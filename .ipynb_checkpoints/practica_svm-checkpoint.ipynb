{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#04B404\"><h1 align=\"center\">Machine Learning 2019-2020</h1></font>\n",
    "<font color=\"#6E6E6E\"><h2 align=\"center\">Práctica 4: Support Vector Machines - El algoritmo SMO</h2></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta práctica vamos a implementar una versión simplificada del algoritmo SMO basada en <a href=\"http://cs229.stanford.edu/materials/smo.pdf\">estas notas</a>. El algoritmo SMO original (<a href=\"https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/\">Platt, 1998</a>) utiliza una heurística algo compleja para seleccionar las alphas con respecto a las cuales optimizar. La versión que vamos a implementar simplifica el proceso de elección de las alphas, a costa de una convergencia más lenta. Una vez implementado, compararemos los resultados de nuestro algoritmo con los obtenidos usando la clase <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\">SVC</a> del paquete *sklearn.svm*.\n",
    "\n",
    "Finalmente, utilizaremos la implementación de *sklearn* para resolver problemas sencillos de clasificación en dos dimensiones y visualizar fácilmente la frontera de decisión y el margen.\n",
    "\n",
    "Todo el código que tienes que desarrollar lo debes incluir en el fichero *svm.py*, en los lugares indicados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrega:\n",
    "\n",
    "La fecha tope para la entrega es el <font color=\"#931405\">**19/12/2019 a las 23:59**</font>. Se debe subir a la plataforma moodle un único fichero comprimido con el siguiente contenido:\n",
    "\n",
    "- El fichero *svm.py* con todo el código añadido para la realización de esta práctica.\n",
    "- Este notebook con las respuestas a las preguntas planteadas al final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implementando los kernels\n",
    "\n",
    "Lo primero que vamos a hacer es implementar las funciones que calculan los kernels. En el fichero *svm.py*, completa el código de las funciones *linear_kernel*, *poly_kernel* y *rbf_kernel*. Luego ejecuta las celdas siguientes, que comparan los resultados de estas funciones con funciones equivalentes de *sklearn*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'svm_sol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-38d3212bb6e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msvm_sol\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpolynomial_kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrbf_kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'svm_sol'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import svm_sol as svm \n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "from sklearn.metrics.pairwise import rbf_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de prueba:\n",
    "n = 10\n",
    "m = 8\n",
    "d = 4\n",
    "x = np.random.randn(n, d)\n",
    "y = np.random.randn(m, d)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Kernel lineal\n",
    "\n",
    "Completa el código de la función *linear_kernel* y comprueba tu solución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con tu implementación:\n",
    "K = svm.linear_kernel(x, y)\n",
    "print(\"El array K deberia tener dimensiones: (%d, %d)\" % (n, m))\n",
    "print(\"A ti te sale un array con dimensiones:\", K.shape)\n",
    "\n",
    "# Con la implementación de sklearn:\n",
    "K_ = polynomial_kernel(x, y, degree=1, gamma=1, coef0=1)\n",
    "\n",
    "# Diferencia entre tu kernel y el de sklearn (deberia salir practicamente 0): \n",
    "maxdif = np.max(np.abs(K - K_))\n",
    "print(\"Maxima diferencia entre tu implementacion y la de sklearn:\", maxdif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Kernel polinómico\n",
    "\n",
    "Completa el código de la función *poly_kernel* y comprueba tu solución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con tu implementación:\n",
    "K = svm.poly_kernel(x, y, deg=2, b=1)\n",
    "print(\"El array K deberia tener dimensiones: (%d, %d)\" % (n, m))\n",
    "print(\"A ti te sale un array con dimensiones:\", K.shape)\n",
    "\n",
    "# Con la implementación de sklearn:\n",
    "K_ = polynomial_kernel(x, y, degree=2, gamma=1, coef0=1)\n",
    "\n",
    "# Diferencia entre tu kernel y el de sklearn (deberia salir practicamente 0): \n",
    "maxdif = np.max(np.abs(K - K_))\n",
    "print(\"Maxima diferencia entre tu implementacion y la de sklearn:\", maxdif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Kernel gausiano\n",
    "\n",
    "Completa el código de la función *rbf_kernel* y comprueba tu solución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1.0\n",
    "\n",
    "# Con tu implementación:\n",
    "K = svm.rbf_kernel(x, y, sigma=s)\n",
    "print(\"El array K deberia tener dimensiones: (%d, %d)\" % (n, m))\n",
    "print(\"A ti te sale un array con dimensiones:\", K.shape)\n",
    "\n",
    "# Con la implementación de sklearn:\n",
    "K_ = rbf_kernel(x, y, gamma=1/(2*s**2))\n",
    "\n",
    "# Diferencia entre tu kernel y el de sklearn (deberia salir practicamente 0): \n",
    "maxdif = np.max(np.abs(K - K_))\n",
    "print(\"Maxima diferencia entre tu implementacion y la de sklearn:\", maxdif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. El algoritmo SMO\n",
    "\n",
    "A continuación vas a completar la clase SVM, que representa un clasificador basado en máquinas de vectores de soporte, y vas a implementar el algoritmo SMO dentro de esta clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Implementa el método *evaluate_model*\n",
    "\n",
    "Lo primero que debes hacer es completar el método *evaluate_model*, que recibe un array con los datos (atributos) del problema $x$ y calcula $f(x)$. Una vez que lo tengas, para comprobar que funciona bien, puedes ejecutar la siguiente celda de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de prueba (problema XOR):\n",
    "x = np.array([[-1, -1], [1, 1], [1, -1], [-1, 1]])\n",
    "y = np.array([1, 1, -1, -1])\n",
    "\n",
    "# Alphas y b:\n",
    "alpha = np.array([0.125, 0.125, 0.125, 0.125])\n",
    "b = 0\n",
    "\n",
    "# Clasificador, introducimos la solucion a mano:\n",
    "svc = svm.SVM(C=1000, kernel=\"poly\", sigma=1, deg=2, b=1)\n",
    "svc.init_model(alpha, b, x, y)\n",
    "\n",
    "# Clasificamos los puntos x:\n",
    "y_ = svc.evaluate_model(x)\n",
    "\n",
    "# Las predicciones deben ser exactamente iguales que las clases:\n",
    "print(\"Predicciones (deberian ser [1, 1, -1, -1]):\", y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Completa el resto de métodos de la clase SVM\n",
    "\n",
    "Completa los métodos *select_alphas*, *calculate_eta*, *update_alphas* y *update_b*. Cuando los tengas acabados, ejecuta las celdas siguientes para comprobar tu implementación.\n",
    "\n",
    "La primera prueba consiste en entrenar el problema del XOR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(svm)\n",
    "\n",
    "# Prueba con los datos del XOR:\n",
    "x = np.array([[-1, -1], [1, 1], [1, -1], [-1, 1]])\n",
    "y = np.array([1, 1, -1, -1])\n",
    "\n",
    "# Clasificador que entrenamos para resolver el problema:\n",
    "svc = svm.SVM(C=1000, kernel=\"poly\", sigma=1, deg=2, b=1)\n",
    "svc.simple_smo(x, y, maxiter = 100, verb=True)\n",
    "\n",
    "# Imprimimos los alphas y el bias (deberian ser alpha_i = 0.125, b = 0):\n",
    "print(\"alpha =\", svc.alpha)\n",
    "print(\"b =\", svc.b)\n",
    "\n",
    "# Clasificamos los puntos x (las predicciones deberian ser iguales a las clases reales):\n",
    "y_ = svc.evaluate_model(x)\n",
    "print(\"Predicciones =\", y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente prueba genera un problema al azar y lo resuelve con tu método y con *sklearn*. Ambas soluciones deberían ser parecidas, aunque la tuya será mucho más lenta. Prueba con los diferentes tipos de kernels para comprobar tu implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prueba con otros problemas y comparacion con sklearn:\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generacion de los datos:\n",
    "n = 20\n",
    "X = np.random.rand(n,2)\n",
    "y = 2.0*(X[:,0] > X[:,1]) -1\n",
    "\n",
    "# Uso de SVC:\n",
    "clf = SVC(C=10.0, kernel='rbf', degree=2.0, coef0=1.0, gamma=0.5)\n",
    "clf.fit(X, y)  \n",
    "print(\"Resultados con sklearn:\")\n",
    "print(\"  -- Num. vectores de soporte =\", clf.dual_coef_.shape[1])\n",
    "print(\"  -- Bias b =\", clf.intercept_[0])\n",
    "\n",
    "# Uso de tu algoritmo:\n",
    "svc = svm.SVM(C=10, kernel=\"rbf\", sigma=1.0, deg=2.0, b=1.0)\n",
    "svc.simple_smo(X, y, maxiter = 500, tol=1.e-15, verb=False, print_every=10)\n",
    "print(\"Resultados con tu algoritmo:\")\n",
    "print(\"  -- Num. vectores de soporte =\", svc.num_sv)\n",
    "print(\"  -- Bias b =\", svc.b)\n",
    "\n",
    "# Comparacion entre las alphas:\n",
    "a1 = clf.dual_coef_\n",
    "a2 = (svc.alpha * y)[svc.is_sv]\n",
    "\n",
    "# Maxima diferencia entre tus alphas y las de sklearn: \n",
    "maxdif = np.max(np.abs(np.sort(a1) - np.sort(a2)))\n",
    "print(\"Maxima diferencia entre tus alphas y las de sklearn:\", maxdif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualización de modelos sencillos\n",
    "\n",
    "Finalmente vamos a utilizar la implementación de *sklearn* para resolver problemas sencillos de clasificación en dos dimensiones. El objetivo es entender cómo funcionan los distintos tipos de kernel (polinómico y RBF) con problemas que se pueden visualizar fácilmente. \n",
    "\n",
    "Para implementar los modelos utilizaremos la clase <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\">SVC</a> del paquete *sklearn.svm*. \n",
    "\n",
    "Primero importamos algunos módulos adicionales, establecemos el modo *inline* para las gráficas de *matplotlib* e inicializamos la semilla del generador de números aleatorios. El módulo *p4_utils* contiene funciones para generar datos en 2D y visualizar los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p4_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline\n",
    "np.random.seed(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda realiza las siguientes acciones:\n",
    "\n",
    "- Crea un problema con dos conjuntos de datos (entrenamiento y test) de 50 puntos cada uno, y dos clases (+1 y -1). La frontera que separa las clases es lineal. \n",
    "\n",
    "- Entrena un clasificador SVC para separar las dos clases, con un kernel lineal.\n",
    "\n",
    "- Imprime los vectores de soporte, las alphas y el bias.\n",
    "\n",
    "- Obtiene la tasa de acierto en training y en test.\n",
    "\n",
    "- Y finalmente dibuja el modelo sobre los datos de entrenamiento y test. La línea negra es la frontera de separación, mientras que las líneas azul y roja representan los márgenes para las clases azul y roja respectivamente. Sobre la gráfica de entrenamiento muestra además los vectores de soporte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del problema, datos de entrenamiento y test:\n",
    "np.random.seed(300)\n",
    "n = 50\n",
    "model = 'linear'\n",
    "ymargin = 0.5\n",
    "x, y = createDataSet(n, model, ymargin)\n",
    "xtest, ytest = createDataSet(n, model, ymargin)\n",
    "\n",
    "# Construcción del clasificador:\n",
    "clf = SVC(C=10, kernel='linear', degree=1.0, coef0=1.0, gamma=0.1)\n",
    "clf.fit(x, y)  \n",
    "\n",
    "# Vectores de soporte:\n",
    "print(\"Vectores de soporte:\")\n",
    "for i in clf.support_:\n",
    "    print(\"   [%f, %f]  c = %d\" % (x[i,0], x[i,1], y[i]))\n",
    "\n",
    "# Coeficientes a_i y b:\n",
    "print(\"Coeficientes a_i:\")\n",
    "print(\"  \", clf.dual_coef_)\n",
    "print(\"Coeficiente b:\")\n",
    "print(\"  \", clf.intercept_[0])\n",
    "\n",
    "# Calculo del acierto en los conjuntos de entrenamiento y test:\n",
    "score_train = clf.score(x, y)\n",
    "print(\"Score train = %f\" % (score_train))\n",
    "score_test = clf.score(xtest, ytest)\n",
    "print(\"Score test = %f\" % (score_test))\n",
    "\n",
    "# Gráficas:\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plotModel(x[:,0],x[:,1],y,clf,\"Training, score = %f\" % (score_train))\n",
    "for i in clf.support_:\n",
    "    if y[i] == -1:\n",
    "        plt.plot(x[i,0],x[i,1],'ro',ms=10)\n",
    "    else:\n",
    "        plt.plot(x[i,0],x[i,1],'bo',ms=10)\n",
    "\n",
    "plt.subplot(122)\n",
    "plotModel(xtest[:,0],xtest[:,1],ytest,clf,\"Test, score = %f\" % (score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Responde a las siguientes cuestiones:\n",
    "\n",
    "**(1)** Escribe la ecuación de la frontera de decisión para este problema como combinación lineal de las funciones de kernel sobre cada uno de los vectores de soporte. Ten en cuenta que los coeficientes $a_{i}$ son el producto de la clase del punto y su multiplicador de Lagrange correspondiente: $a_{i} = \\alpha_{i} t_{i}$. Por este motivo encontramos valores negativos.\n",
    "\n",
    "- <font color=\"#931405\">Tu respuesta aquí</font>\n",
    "\n",
    "**(2)** Con el conjunto de datos anterior, prueba a entrenar el clasificador con un parámetro *C* igual a 0.01. ¿Se siguen clasificando bien todos los patrones? ¿Qué ocurre con el margen? ¿Qué pasa si bajas aún más el valor de *C* hasta 0.001? ¿Qué pasa con el margen si aumentamos mucho el parámetro C? Razona tus respuestas.\n",
    "\n",
    "- <font color=\"#931405\">Tu respuesta aquí</font>\n",
    "\n",
    "**(3)** Genera un nuevo conjunto de datos manteniendo el modelo lineal pero cambiando el parámetro *ymargin* a -0.5. Como ves ahora el problema ya no es linealmente separable. Prueba a resolverlo con el clasificador inicial (con *C=10*) y luego prueba otros valores de C. ¿Qué ocurre con valores altos de *C*? ¿Y con valores bajos? ¿Qué ocurre con los vectores de soporte? \n",
    "\n",
    "- <font color=\"#931405\">Tu respuesta aquí</font>\n",
    "\n",
    "**(4)** Haz pruebas utilizando un kernel gausiano y variando los parámetros *C* y *gamma*. Comenta los resultados.\n",
    "\n",
    "- <font color=\"#931405\">Tu respuesta aquí</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
